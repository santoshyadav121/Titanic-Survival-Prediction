# -*- coding: utf-8 -*-
"""titanic_survival.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11l6b-Cy-9CbpzWhiXXl3TqkTJn3nB_En
"""
# pip install imblearn
# Importing all the libraries we need for model development
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold
from sklearn import linear_model, metrics

from  sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import StandardScaler, LabelEncoder,OneHotEncoder
from collections import Counter
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline,make_pipeline
from sklearn.manifold import TSNE
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
import os 
import pickle

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)

"""## **Observing the dataset**"""

# Loading titanic data into dataframe
df_load = sns.load_dataset('titanic')
df_load

"""Just by looking the dataset we can conclude various points:-

*   Pclass and Class column both are representing the same information so we can say there is redundancy, since Pclass data is in integer hence we can keep it and remove Class column.
*   Similarly embarked and embark_town here we can remove any one lets say embarked.

*   Similarly survived and alive column here we can remove alive column.
*   Similarly sex and adult_male column here we can remove adult_male column.

*   whenever sibsp and parch is zero ie. whenever the person has no siblings or parents or child he is traveling alone , so we can say alone column is a subset of sibsp and parch column so we can remove alone column also









"""

df = df_load[[	'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare','embark_town','who','deck', 'survived']]

"""## **Handling missing dataset**

lets check which column have how many missing data
"""

df.isnull().sum()

"""from this we can conclude some points:-

*   we have total 891 dataset and deck column more than 70% of data missing imputing such large data is not a very good idea hence we will drop deck column
*   embark_town has only 2 missing data since the number is very small removing this 2 row will not affect the dataset hence we wil remove such rows.

*   age column have 177 missing values since the number is not so high not so low we can neither remove rows nor remove column also it is very important feature hence we will ampute null values.
"""

df = df.dropna(subset = ['embark_town'] )
df = df.drop(columns='deck', axis = 1)

"""now we have handled embark and deck column lets ampute age column .  
Amputation of numerical values can take place in 2 ways if the distribution of data is normal or there is no significant outliers we can use mean amputation but if there are outliers or data is left or right skewed median amputatation is better. lets check!
"""

fig, ax = plt.subplots(1,2, figsize = (15,5))
sns.distplot(df['age'], ax =  ax[0])
sns.boxplot(y = df['age'], ax = ax[1])

"""As we can see distribution plot of age is slightly left skewed it means there are some outliers ,same thing can be seen in boxplot hence we should use median amputation to handle the missing values of age column."""

df['age'] = df['age'].fillna(df.groupby('pclass')['age'].transform('median') )
df.isnull().sum()

"""Hurray!! now we have handled all missing values lets move on to exploratory data analysis(EDA) ie. lets make some visual analysis

**Surival Analysis**
"""

fig, ax = plt.subplots(1,2,figsize = (20,4))
sns.countplot(data = df, x = 'sex', hue = 'survived', ax = ax[0])
ax[0].legend(title = 'survived',loc='upper right', bbox_to_anchor=(1,1))
for p in ax[0].patches:
    height = p.get_height()
    ax[0].text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha="center" ,color = 'Black',fontsize = 12)
    
sns.countplot(data = df, x = 'who', hue = 'survived', ax = ax[1])
ax[1].legend(title = 'survived',loc='upper right', bbox_to_anchor=(1,1))
for p in ax[1].patches:
    height = p.get_height()
    ax[1].text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha="center" ,color = 'Black',fontsize = 12)

"""
*   this  count plot shows that only 109 males survived on the other side 233 females have survived which is 2 times of males survived
also it shows that only 81 females died  on the other side 468 males died which is 6 times of females died.

*   Also we can see countplot of man, woman and child where we can see more than 50% of people died are man  where survival rate of woman and child are more than death rate.

"""

plt.figure(figsize = (10,6))
ax = sns.countplot(data = df, x = 'pclass', hue = 'survived',)
for p in ax.patches:
  height = p.get_height()
  ax.text(p.get_x()+p.get_width()/2., height + .1,height ,ha="center" ,color = 'Black',fontsize = 14)

"""this count plot shows that in class 3 more people have died than class 1&2 
also in class 1 more people have survived than class 2&3
"""

fig, ax = plt.subplots(1,2,figsize = (20,4))
sns.countplot(data = df, x = 'sibsp', hue = 'survived', ax = ax[0])
ax[0].legend(title = 'survived',loc='upper right', bbox_to_anchor=(1,1))
for p in ax[0].patches:
    height = p.get_height()
    ax[0].text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha="center" ,color = 'Black',fontsize = 12)
    
sns.countplot(data = df, x = 'parch', hue = 'survived', ax = ax[1])
ax[1].legend(title = 'survived',loc='upper right', bbox_to_anchor=(1,1))
for p in ax[1].patches:
    height = p.get_height()
    ax[1].text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha="center" ,color = 'Black',fontsize = 12)

"""
*   Here we can see the no. of siblings aboarded the ship from each family and people with no siblings are likely to be less survived than people with 1 or more siblings.
*   similarly we can see people with children or parents aboarded and people with no children or parents are likely to be less survived than people with with 1 or more children or parents.
 """

plt.figure(figsize = (10,7))
sns.relplot(data=df, y='fare', x = 'age',hue = 'pclass', palette = plt.cm.Set1 )

"""This scatter plot shows that class 1 fares are higher than 2&3 but class 3 
fares are highly varible for range of ages than class 2&3 fares where fares are mostly stable and const for ranges of ages
"""

fig, ax = plt.subplots(1,2,figsize = (25,4))
sns.boxplot(data = df , x = df['age'], ax = ax[0])
sns.boxplot(data = df , x = df['fare'], ax = ax[1])

"""In this plot we can see that both have enough ouliers but there are more outliers in fare than age.

## **Converting Category values**
"""

df.head()

"""

*   Since sex, embark_town and who column have categorical values we will convert them into binary vectors using  get dummies .

*  Also we are converting Pclass into binary vector although it is numerical value, since that value is not ordinal means pclass 1 is not less than pclass 3 both have no relation hence we need to categories them into binary vectors.

"""

df_cat = pd.get_dummies(df, columns= ['sex','embark_town','pclass','who'])
df_cat.head()

"""## **Preparing Data**"""

# Seperating the data and target variable
label = df_cat['survived'].values
data = df_cat.drop(columns = 'survived').values

# dividing the data into 80-20 train test dataset
x_train, x_test, y_train, y_test = train_test_split(data,label, test_size = .2, random_state = 42)

# stadardising the dataset to remove the scaling factor
std = StandardScaler()
x_train_std = std.fit_transform(x_train)
x_test_std = std.transform(x_test)

Counter(label)

# using various models to compare which one is giving better result
knn = KNeighborsClassifier()
log = LogisticRegressionCV(max_iter = 500, random_state=42)
svc_lin = SVC(kernel = 'linear', random_state=42)
svc_rbf = SVC(kernel = 'rbf', random_state=42)
gnb = GaussianNB()
tree = DecisionTreeClassifier(random_state = 42)
forest = RandomForestClassifier(random_state=42)

"""## **Data Prediction**"""

classifier = [knn, log, svc_lin, svc_rbf, gnb, tree, forest] 
for item in classifier:
  item.fit(x_train_std,y_train)
  y_true =  y_train
  y_pred = item.predict(x_train_std)
  print(f'the model is {item} ')
  print(f'Accuracy : {metrics.accuracy_score(y_true, y_pred)}, F1-score : {metrics.f1_score(y_true,y_pred)}, roc_auc : {metrics.roc_auc_score(y_true,y_pred)}')

"""performance measures on train data shows that  decision tree and random forest perform better """

classifier = [knn, log, svc_lin, svc_rbf, gnb, tree, forest] 
for item in classifier:
  item.fit(x_train_std,y_train)
  y_true =  y_test
  y_pred = item.predict(x_test_std)
  print(f'the model is {item} ')
  print(f'Accuracy : {metrics.accuracy_score(y_true, y_pred)}, F1-score : {metrics.f1_score(y_true,y_pred)}, roc_auc : {metrics.roc_auc_score(y_true,y_pred)}')

"""performance measures on test data shows that logistic regression perform better but
since decision tree performance on test is also closer to logistic regression and also it perform far better in train data we will go with decision tree
"""

tree.fit(x_train_std,y_train)
y_true =  y_test
y_pred = tree.predict(x_test_std)
print(f'Accuracy : {metrics.accuracy_score(y_true, y_pred)}, F1-score : {metrics.f1_score(y_true,y_pred)}, roc_auc : {metrics.roc_auc_score(y_true,y_pred)}')

"""Since data was highly imbalanced hence we are using f1_score and roc curve parameters for performance

1st graph shows the metrics parameters ie. precision, recall and f1_score and the 2nd graph shows roc_auc curve and its value
"""

fig , ax = plt.subplots(1,2, figsize = (15,5))
report = metrics.classification_report(y_true, y_pred, output_dict=True, target_names=set(label))
sns.heatmap(pd.DataFrame(report).iloc[:-1,:-3].T,  cmap = plt.cm.RdYlBu, cbar = False , annot=True, ax = ax[0])
metrics.plot_roc_curve(tree,x_test,y_test, ax = ax[1])

"""In order to check how our model performs on tst data we can plot confusion matrix where diagonal values are correctly classified datasets

"""

cm = metrics.confusion_matrix(y_true, y_pred)
plt.figure(figsize = (10,7))
sns.heatmap(cm, xticklabels=set(label), yticklabels=set(label), fmt= 'd', cmap = plt.cm.Blues, cbar = False , annot=True)
plt.ylabel('Actual Value')
plt.xlabel('Predicted Value')
plt.title('Actual vs predicted value on test data')

"""In order to visualize decision decision tree here we are reducing the dimension of data into 2 features so that we can use meshplot

"""

x = TSNE(n_components=2, random_state= 42).fit_transform(x_train)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(x,y_train)

x_min,x_max = x[:,0].min()-1, x[:,0].max()+1
y_min,y_max = x[:,1].min()-1, x[:,1].max()+1
h = .09
xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))

Z = clf.predict(np.c_[xx.ravel(),yy.ravel()])

plt.figure(figsize = (10,7))
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx,yy,Z,cmap = plt.cm.Set3)

# scatterplot for data points
plt.scatter(x[:,0],x[:,1], c =y_train, cmap = plt.cm.flag)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Decision Tree")
plt.show()

"""## **Model Deployment Procedure**

Whatever we have done above, in order to deploy it we need to make a pipeline where whole procedure happens in one go so lets make it!
"""

df.head()

lb = df['survived'].values
dt = df.drop(columns='survived').values
X_train,X_test,Y_train,Y_test = train_test_split(dt,lb,test_size=.2,random_state=42)

X_train

# define model
model = DecisionTreeClassifier(random_state=42)
# define transform
transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown = 'ignore'), [0,1, 6,7])])
# define pipeline
pipe = Pipeline(steps=[('t', transformer), ('m',model)])
# fit the pipeline on the transformed data
pipe.fit(X_train, Y_train)

Y_true =  Y_test
Y_pred = pipe.predict(X_test)
print(f'Accuracy : {metrics.accuracy_score(Y_true, Y_pred)}, F1-score : {metrics.f1_score(Y_true,Y_pred)}, roc_auc : {metrics.roc_auc_score(Y_true,Y_pred)}')

pickle.dump(pipe, open('titanic_survival.pkl','wb'))

model = pickle.load(open('titanic_survival.pkl','rb'))

# model.predict(X_test[30].reshape(1,-1))
if model.predict(X_test[40].reshape(1,-1))[0] == 0:
  print('ohh!! no , this person didn\'t survived')
else:
  print('congrats!!! this person survived titanic incident')

